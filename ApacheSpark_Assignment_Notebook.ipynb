{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "config = SparkConf().setAppName(\"SparkAssignment\").setMaster(\"local\")\n",
    "sc = SparkContext(conf = config)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('data_processing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('sample_data.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ratings', 'age', 'experience', 'family', 'mobile']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ratings: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: double (nullable = true)\n",
      " |-- family: integer (nullable = true)\n",
      " |-- mobile: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+\n",
      "|ratings|age|experience|family| mobile|\n",
      "+-------+---+----------+------+-------+\n",
      "|      3| 32|       9.0|     3|   Vivo|\n",
      "|      3| 27|      13.0|     3|  Apple|\n",
      "|      4| 22|       2.5|     0|Samsung|\n",
      "|      4| 37|      16.5|     4|  Apple|\n",
      "|      5| 27|       9.0|     1|     MI|\n",
      "+-------+---+----------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| mobile|\n",
      "+-------+\n",
      "|     MI|\n",
      "|   Oppo|\n",
      "|Samsung|\n",
      "|   Vivo|\n",
      "|  Apple|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('mobile').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=df.drop('mobile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|ratings|age|experience|family|\n",
      "+-------+---+----------+------+\n",
      "|      3| 32|       9.0|     3|\n",
      "|      3| 27|      13.0|     3|\n",
      "|      4| 22|       2.5|     0|\n",
      "|      4| 37|      16.5|     4|\n",
      "|      5| 27|       9.0|     1|\n",
      "|      4| 27|       9.0|     0|\n",
      "|      5| 37|      23.0|     5|\n",
      "|      5| 37|      23.0|     5|\n",
      "|      3| 22|       2.5|     0|\n",
      "|      3| 27|       6.0|     0|\n",
      "+-------+---+----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_new = sqlContext.createDataFrame(df_new)\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "scaler = StandardScaler( withStd=True, withMean=False)\n",
    "rdd = df_new.rdd.map(lambda data: Vectors.dense([float(c) for c in data]))\n",
    "scalerModel = scaler.fit(rdd)\n",
    "scaledData = scalerModel.transform(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "mat = RowMatrix(scaledData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = mat.computePrincipalComponents(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected = mat.multiply(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([2.6813, 5.1736, 1.3293, 1.6262]),\n",
       " DenseVector([2.6813, 4.3652, 1.92, 1.6262]),\n",
       " DenseVector([3.575, 3.5568, 0.3692, 0.0]),\n",
       " DenseVector([3.575, 5.982, 2.437, 2.1682]),\n",
       " DenseVector([4.4688, 4.3652, 1.3293, 0.5421]),\n",
       " DenseVector([3.575, 4.3652, 1.3293, 0.0]),\n",
       " DenseVector([4.4688, 5.982, 3.397, 2.7103]),\n",
       " DenseVector([4.4688, 5.982, 3.397, 2.7103]),\n",
       " DenseVector([2.6813, 3.5568, 0.3692, 0.0]),\n",
       " DenseVector([2.6813, 4.3652, 0.8862, 0.0]),\n",
       " DenseVector([1.7875, 4.3652, 0.8862, 1.0841]),\n",
       " DenseVector([4.4688, 4.3652, 0.8862, 1.0841]),\n",
       " DenseVector([2.6813, 5.982, 2.437, 2.7103]),\n",
       " DenseVector([4.4688, 4.3652, 0.8862, 0.0]),\n",
       " DenseVector([3.575, 3.5568, 0.8862, 0.5421]),\n",
       " DenseVector([3.575, 5.982, 1.3293, 1.0841]),\n",
       " DenseVector([3.575, 4.3652, 0.8862, 0.5421]),\n",
       " DenseVector([0.8938, 5.982, 3.397, 2.7103]),\n",
       " DenseVector([1.7875, 6.7903, 3.397, 1.0841]),\n",
       " DenseVector([3.575, 5.982, 0.8862, 0.0]),\n",
       " DenseVector([4.4688, 3.5568, 0.3692, 0.0]),\n",
       " DenseVector([2.6813, 5.982, 2.437, 2.7103]),\n",
       " DenseVector([2.6813, 6.7903, 3.397, 2.7103]),\n",
       " DenseVector([1.7875, 4.3652, 1.3293, 1.0841]),\n",
       " DenseVector([3.575, 4.3652, 0.8862, 0.5421]),\n",
       " DenseVector([4.4688, 4.3652, 0.3692, 0.0]),\n",
       " DenseVector([1.7875, 4.3652, 0.8862, 1.0841]),\n",
       " DenseVector([4.4688, 5.982, 1.92, 0.5421]),\n",
       " DenseVector([1.7875, 5.1736, 2.437, 1.0841]),\n",
       " DenseVector([2.6813, 4.3652, 0.8862, 0.0]),\n",
       " DenseVector([2.6813, 4.3652, 0.8862, 0.0]),\n",
       " DenseVector([3.575, 3.5568, 0.8862, 0.5421]),\n",
       " DenseVector([3.575, 5.982, 0.8862, 0.0])]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.rows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([3.8573, -4.0192]),\n",
       " DenseVector([3.7754, -3.8952]),\n",
       " DenseVector([1.3141, -4.3172]),\n",
       " DenseVector([5.0474, -5.2179]),\n",
       " DenseVector([2.4167, -5.4998]),\n",
       " DenseVector([2.3153, -4.6089]),\n",
       " DenseVector([5.7178, -6.2126]),\n",
       " DenseVector([5.7178, -6.2126]),\n",
       " DenseVector([1.5156, -3.4545]),\n",
       " DenseVector([2.2541, -3.6983]),\n",
       " DenseVector([3.0612, -2.8924]),\n",
       " DenseVector([2.4568, -5.4803]),\n",
       " DenseVector([5.5516, -4.3836]),\n",
       " DenseVector([1.8512, -5.4236]),\n",
       " DenseVector([1.9234, -4.4014]),\n",
       " DenseVector([3.7851, -5.0414]),\n",
       " DenseVector([2.3555, -4.5893]),\n",
       " DenseVector([6.5237, -2.7622]),\n",
       " DenseVector([5.8459, -3.7277]),\n",
       " DenseVector([2.9168, -4.9368]),\n",
       " DenseVector([1.1127, -5.1798]),\n",
       " DenseVector([5.5516, -4.3836]),\n",
       " DenseVector([6.5528, -4.6753]),\n",
       " DenseVector([3.3239, -2.9403]),\n",
       " DenseVector([2.3555, -4.5893]),\n",
       " DenseVector([1.5447, -5.3677]),\n",
       " DenseVector([3.0612, -2.8924]),\n",
       " DenseVector([3.631, -5.9395]),\n",
       " DenseVector([4.4126, -3.248]),\n",
       " DenseVector([2.2541, -3.6983]),\n",
       " DenseVector([2.2541, -3.6983]),\n",
       " DenseVector([1.9234, -4.4014]),\n",
       " DenseVector([2.9168, -4.9368])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projected.rows.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = mat.computeSVD(2, computeU=True)\n",
    "U = svd.U       # The U factor is a RowMatrix.\n",
    "s = svd.s       # The singular values are stored in a local dense vector.\n",
    "V = svd.V       # The V factor is a local dense matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([-0.171, -0.061]),\n",
       " DenseVector([-0.1573, -0.0823]),\n",
       " DenseVector([-0.1327, 0.2082]),\n",
       " DenseVector([-0.2124, -0.1051]),\n",
       " DenseVector([-0.1731, 0.1732]),\n",
       " DenseVector([-0.1578, 0.134]),\n",
       " DenseVector([-0.2347, -0.1251]),\n",
       " DenseVector([-0.2347, -0.1251]),\n",
       " DenseVector([-0.12, 0.1338]),\n",
       " DenseVector([-0.1418, 0.0869]),\n",
       " DenseVector([-0.1344, -0.0578]),\n",
       " DenseVector([-0.1725, 0.1652]),\n",
       " DenseVector([-0.2023, -0.2147]),\n",
       " DenseVector([-0.1672, 0.2356]),\n",
       " DenseVector([-0.1391, 0.1412]),\n",
       " DenseVector([-0.1991, 0.0334]),\n",
       " DenseVector([-0.1571, 0.1261]),\n",
       " DenseVector([-0.184, -0.4224]),\n",
       " DenseVector([-0.2068, -0.2576]),\n",
       " DenseVector([-0.1906, 0.1311]),\n",
       " DenseVector([-0.1454, 0.2825]),\n",
       " DenseVector([-0.2023, -0.2147]),\n",
       " DenseVector([-0.2274, -0.2888]),\n",
       " DenseVector([-0.1376, -0.0851]),\n",
       " DenseVector([-0.1571, 0.1261]),\n",
       " DenseVector([-0.1634, 0.2674]),\n",
       " DenseVector([-0.1344, -0.0578]),\n",
       " DenseVector([-0.2134, 0.1066]),\n",
       " DenseVector([-0.1637, -0.1683]),\n",
       " DenseVector([-0.1418, 0.0869]),\n",
       " DenseVector([-0.1418, 0.0869]),\n",
       " DenseVector([-0.1391, 0.1412]),\n",
       " DenseVector([-0.1906, 0.1311])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.rows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35.91681067,  8.09251578])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.50961817, -0.8008306 , -0.26199377, -0.17412331,  0.67311762,\n",
       "       -0.15120319, -0.49786969, -0.52552454])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.format(\"libsvm\").load(\"kmeans_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|           (3,[],[])|\n",
      "|  1.0|(3,[0,1,2],[0.1,0...|\n",
      "|  2.0|(3,[0,1,2],[0.2,0...|\n",
      "|  3.0|(3,[0,1,2],[9.0,9...|\n",
      "|  4.0|(3,[0,1,2],[9.1,9...|\n",
      "|  5.0|(3,[0,1,2],[9.2,9...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[0.1 0.1 0.1]\n",
      "[9.1 9.1 9.1]\n"
     ]
    }
   ],
   "source": [
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(label=0.0, features=SparseVector(3, {}), prediction=0), Row(label=1.0, features=SparseVector(3, {0: 0.1, 1: 0.1, 2: 0.1}), prediction=0), Row(label=2.0, features=SparseVector(3, {0: 0.2, 1: 0.2, 2: 0.2}), prediction=0)]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(dataset)\n",
    "rows = predictions.collect()\n",
    "print(rows[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9997530305375207\n"
     ]
    }
   ],
   "source": [
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fpgrowth and prefix span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(\"fpgrowth_data.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['r', 'z', 'h', 'k', 'p'],\n",
       " ['z', 'y', 'x', 'w', 'v', 'u', 't', 's'],\n",
       " ['s', 'x', 'o', 'n', 'r'],\n",
       " ['x', 'z', 'y', 'm', 't', 's', 'q', 'e'],\n",
       " ['z'],\n",
       " ['x', 'z', 'y', 'r', 'q', 't', 'p']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "transactions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "model = FPGrowth.train(data=transactions, minSupport=0.2, numPartitions= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqItemset(items=['z'], freq=5)\n",
      "FreqItemset(items=['x'], freq=4)\n",
      "FreqItemset(items=['x', 'z'], freq=3)\n",
      "FreqItemset(items=['y'], freq=3)\n",
      "FreqItemset(items=['y', 'x'], freq=3)\n",
      "FreqItemset(items=['y', 'x', 'z'], freq=3)\n",
      "FreqItemset(items=['y', 'z'], freq=3)\n",
      "FreqItemset(items=['r'], freq=3)\n",
      "FreqItemset(items=['r', 'x'], freq=2)\n",
      "FreqItemset(items=['r', 'z'], freq=2)\n",
      "FreqItemset(items=['s'], freq=3)\n",
      "FreqItemset(items=['s', 'y'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'x'], freq=3)\n",
      "FreqItemset(items=['s', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'z'], freq=2)\n",
      "FreqItemset(items=['t'], freq=3)\n",
      "FreqItemset(items=['t', 'y'], freq=3)\n",
      "FreqItemset(items=['t', 'y', 'x'], freq=3)\n",
      "FreqItemset(items=['t', 'y', 'x', 'z'], freq=3)\n",
      "FreqItemset(items=['t', 'y', 'z'], freq=3)\n",
      "FreqItemset(items=['t', 's'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'y'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'x'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'z'], freq=2)\n",
      "FreqItemset(items=['t', 'x'], freq=3)\n",
      "FreqItemset(items=['t', 'x', 'z'], freq=3)\n",
      "FreqItemset(items=['t', 'z'], freq=3)\n",
      "FreqItemset(items=['p'], freq=2)\n",
      "FreqItemset(items=['p', 'r'], freq=2)\n",
      "FreqItemset(items=['p', 'r', 'z'], freq=2)\n",
      "FreqItemset(items=['p', 'z'], freq=2)\n",
      "FreqItemset(items=['q'], freq=2)\n",
      "FreqItemset(items=['q', 'y'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'y'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'z'], freq=2)\n"
     ]
    }
   ],
   "source": [
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|items                   |\n",
      "+------------------------+\n",
      "|[r, z, h, k, p]         |\n",
      "|[z, y, x, w, v, u, t, s]|\n",
      "|[s, x, o, n, r]         |\n",
      "|[x, z, y, m, t, s, q, e]|\n",
      "|[z]                     |\n",
      "|[x, z, y, r, q, t, p]   |\n",
      "+------------------------+\n",
      "\n",
      "+---------+----+\n",
      "|    items|freq|\n",
      "+---------+----+\n",
      "|      [s]|   3|\n",
      "|   [s, x]|   3|\n",
      "|[s, x, z]|   2|\n",
      "|   [s, z]|   2|\n",
      "|      [r]|   3|\n",
      "+---------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+----------+----------+\n",
      "|antecedent|consequent|confidence|\n",
      "+----------+----------+----------+\n",
      "|    [t, s]|       [y]|       1.0|\n",
      "|    [t, s]|       [x]|       1.0|\n",
      "|    [t, s]|       [z]|       1.0|\n",
      "|       [p]|       [r]|       1.0|\n",
      "|       [p]|       [z]|       1.0|\n",
      "+----------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = (spark.read.text(\"fpgrowth_data.txt\").select(split(\"value\",\"\\s+\").alias(\"items\")))\n",
    "#.select(split(\"value\", \"\\s+\").alias(\"items\")))\n",
    "data.show(truncate=False)\n",
    "\n",
    "fp = FPGrowth(minSupport=0.3, minConfidence=0.9)\n",
    "fpm = fp.fit(data)\n",
    "fpm.freqItemsets.show(5)\n",
    "fpm.associationRules.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefix Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.fpm import PrefixSpan\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a']], 3\n",
      "[['a'], ['a']], 1\n",
      "[['a'], ['b']], 1\n",
      "[['a'], ['b'], ['a']], 1\n",
      "[['a'], ['b'], ['b']], 1\n",
      "[['a'], ['b'], ['b', 'a']], 1\n",
      "[['a'], ['b', 'a']], 1\n",
      "[['a'], ['b', 'c']], 1\n",
      "[['a'], ['b', 'c'], ['a']], 1\n",
      "[['a'], ['b', 'c'], ['b']], 1\n",
      "[['a'], ['b', 'c'], ['b', 'a']], 1\n",
      "[['a'], ['c']], 2\n",
      "[['a'], ['c'], ['a']], 1\n",
      "[['a'], ['c'], ['b']], 1\n",
      "[['a'], ['c'], ['b', 'a']], 1\n",
      "[['a'], ['e']], 1\n",
      "[['b']], 3\n",
      "[['b'], ['a']], 1\n",
      "[['b'], ['b']], 1\n",
      "[['b'], ['b', 'a']], 1\n",
      "[['b'], ['c']], 1\n",
      "[['b'], ['e']], 1\n",
      "[['b', 'a']], 3\n",
      "[['b', 'a'], ['c']], 1\n",
      "[['b', 'a'], ['e']], 1\n",
      "[['b', 'c']], 1\n",
      "[['b', 'c'], ['a']], 1\n",
      "[['b', 'c'], ['b']], 1\n",
      "[['b', 'c'], ['b', 'a']], 1\n",
      "[['c']], 2\n",
      "[['c'], ['a']], 1\n",
      "[['c'], ['b']], 1\n",
      "[['c'], ['b', 'a']], 1\n",
      "[['e']], 1\n",
      "[['f']], 1\n"
     ]
    }
   ],
   "source": [
    "data = [  [[\"a\", \"b\"], [\"c\"]],\n",
    "      [[\"a\"], [\"c\", \"b\"], [\"a\", \"b\"]],\n",
    "      [[\"a\", \"b\"], [\"e\"]],\n",
    "      [[\"f\"]]]\n",
    "rdd = sc.parallelize(data, 2)\n",
    "\n",
    "model = PrefixSpan.train(rdd)\n",
    "result = model.freqSequences().collect()\n",
    "for fs in sorted(result):\n",
    "    print('{}, {}'.format(fs.sequence,fs.freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.format(\"libsvm\").load(\"classification_decisiontree.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=2).fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       1.0|         1.0|(4,[0,1,2,3],[4.4...|\n",
      "|       1.0|         1.0|(4,[0,1,2,3],[4.5...|\n",
      "|       1.0|         1.0|(4,[0,1,2,3],[4.6...|\n",
      "|       1.0|         1.0|(4,[0,1,2,3],[4.8...|\n",
      "|       1.0|         1.0|(4,[0,1,2,3],[4.8...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0444444 \n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_45608310569fafef850e) of depth 5 with 15 nodes\n"
     ]
    }
   ],
   "source": [
    "treeModel = model.stages[2]\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
