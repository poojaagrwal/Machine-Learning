{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving nips papers from http://papers.nips.cc and extracting only title and abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_url  = \"http://papers.nips.cc\"\n",
    "\n",
    "index_urls = {1987: \"https://papers.nips.cc/book/neural-information-processing-systems-1987\"}\n",
    "for i in range(1,30):\n",
    "    year = i+1987\n",
    "    index_urls[year] = \"http://papers.nips.cc/book/advances-in-neural-information-processing-systems-%d-%d\" % (i, year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 Papers Found\n",
      "94 Papers Found\n",
      "101 Papers Found\n",
      "143 Papers Found\n",
      "144 Papers Found\n",
      "127 Papers Found\n",
      "158 Papers Found\n",
      "140 Papers Found\n",
      "152 Papers Found\n",
      "152 Papers Found\n",
      "150 Papers Found\n",
      "151 Papers Found\n",
      "150 Papers Found\n",
      "152 Papers Found\n",
      "197 Papers Found\n",
      "207 Papers Found\n",
      "198 Papers Found\n",
      "207 Papers Found\n",
      "207 Papers Found\n",
      "204 Papers Found\n",
      "217 Papers Found\n",
      "250 Papers Found\n",
      "262 Papers Found\n",
      "292 Papers Found\n",
      "306 Papers Found\n",
      "368 Papers Found\n",
      "360 Papers Found\n",
      "411 Papers Found\n",
      "403 Papers Found\n",
      "569 Papers Found\n"
     ]
    }
   ],
   "source": [
    "papers = list()\n",
    "for year in sorted(index_urls.keys()):\n",
    "    index_url = index_urls[year]\n",
    "    index_html_path = os.path.join(\"working\", \"html\", str(year)+\".html\")\n",
    "\n",
    "    if not os.path.exists(index_html_path):\n",
    "        r = requests.get(index_url)\n",
    "        if not os.path.exists(os.path.dirname(index_html_path)):\n",
    "            os.makedirs(os.path.dirname(index_html_path))\n",
    "        with open(index_html_path, \"wb\") as index_html_file:\n",
    "            index_html_file.write(r.content)\n",
    "    with open(index_html_path, \"rb\") as f:\n",
    "        html_content = f.read()\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "    paper_links = [link for link in soup.find_all('a') if link[\"href\"][:7]==\"/paper/\"]\n",
    "    print(\"%d Papers Found\" % len(paper_links))\n",
    "\n",
    "\n",
    "    temp_path = os.path.join(\"working\", \"temp.txt\")\n",
    "\n",
    "    for link in paper_links:\n",
    "        paper_title = link.contents[0]\n",
    "        info_link = base_url + link[\"href\"]\n",
    "        #pdf_link = info_link + \".pdf\"\n",
    "        pdf_name = link[\"href\"][7:] + \".pdf\"\n",
    "        paper_id = re.findall(r\"^(\\d+)-\", pdf_name)[0]\n",
    "        #print(year, \" \", paper_id) #paper_title.encode('ascii', 'namereplace')\n",
    "\n",
    "        paper_info_html_path = os.path.join(\"working\", \"html\", str(year), str(paper_id)+\".html\")\n",
    "        if not os.path.exists(paper_info_html_path):\n",
    "            r = requests.get(info_link)\n",
    "            if not os.path.exists(os.path.dirname(paper_info_html_path)):\n",
    "                os.makedirs(os.path.dirname(paper_info_html_path))\n",
    "            with open(paper_info_html_path, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "                \n",
    "        with open(paper_info_html_path, \"rb\") as f:\n",
    "            html_content = f.read()\n",
    "           \n",
    "        paper_soup = BeautifulSoup(html_content, \"lxml\")\n",
    "        try: \n",
    "            abstract = paper_soup.find('p', attrs={\"class\": \"abstract\"}).contents[0]\n",
    "        except:\n",
    "            print(\"Abstract not found %s\" % paper_title.encode(\"ascii\", \"replace\"))\n",
    "            abstract = \"\"\n",
    "\n",
    "\n",
    "        papers.append([paper_title, abstract,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(papers, columns=[ \"title\", \"abstract\"]).to_csv(\"output/papers.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
